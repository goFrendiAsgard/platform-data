version: "3.8"

networks:
  airbyte_public:
  airbyte_internal:
  airflow_internal:
  redpanda_internal:
  metabase_internal:
  data_platform:

configs:
  airbyte_flags:
    file: ./airbyte/flags.yml

volumes:
  airbyte_workspace:
    name: ${AIRBYTE_WORKSPACE_DOCKER_MOUNT:-airbyte_workspace}
  # the data volume is only needed for backward compatibility; when users upgrade
  # from an old Airbyte version that relies on file-based configs, the server needs
  # to read this volume to copy their configs to the database
  airbyte_data:
    name: ${AIRBYTE_DATA_DOCKER_MOUNT:-airbyte_data}
  airbyte_db:
    name: ${AIRBYTE_DB_DOCKER_MOUNT:-airbyte_db}

x-airbyte-logging: &default-airbyte-logging
  options:
    max-size: "100m"
    max-file: "5"
  driver: json-file

x-airflow-common: &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.6.3}
  build: images/airflow
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    # For backward compatibility, with Airflow <2.3
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@airflow-postgres/airflow
    AIRFLOW__CELERY__BROKER_URL: redis://:@airflow-redis:6379/0
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    # yamllint disable rule:line-length
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
    # for other purpose (development, test and especially production usage) build/extend Airflow image.
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
  volumes:
  - ${AIRFLOW_PROJ_DIR:-./airflow}/dags:/opt/airflow/dags
  - ${AIRFLOW_PROJ_DIR:-./airflow}/logs:/opt/airflow/logs
  - ${AIRFLOW_PROJ_DIR:-./airflow}/config:/opt/airflow/config
  - ${AIRFLOW_PROJ_DIR:-./airflow}/plugins:/opt/airflow/plugins
  user: "${AIRFLOW_UID:-50000}:0"
  networks:
  - airflow_internal
  depends_on: &airflow-common-depends-on
    airflow-redis:
      condition: service_healthy
    airflow-postgres:
      condition: service_healthy


services:


  #############################################################################
  # REDPANDA
  #############################################################################
  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:v22.3.11
    hostname: redpanda
    container_name: ${CONTAINER_PREFIX:-my}-redpanda
    # Redpanda docker configuration: https://docs.redpanda.com/docs/reference/docker-compose/#configure-redpanda-in-docker
    # Redpanda node configuration example: https://docs.redpanda.com/docs/reference/node-configuration-sample/
    command:
    - redpanda
    - start
    - --kafka-addr PLAINTEXT://0.0.0.0:29092,OUTSIDE://0.0.0.0:9092
    - --advertise-kafka-addr PLAINTEXT://redpanda:29092,OUTSIDE://localhost:9092
    - --pandaproxy-addr PLAINTEXT://0.0.0.0:28082,OUTSIDE://0.0.0.0:8082
    - --advertise-pandaproxy-addr PLAINTEXT://redpanda:28082,OUTSIDE://localhost:8082
    - --rpc-addr 0.0.0.0:33145
    - --advertise-rpc-addr redpanda:33145
    - --smp 1
    - --memory 1G
    - --mode dev-container
    - --default-log-level=warn
    - --set sasl_mechanism=${KAFKA_SASL_MECHANISM}
    - --set sasl_username=${KAFKA_SASL_USER}
    - --set sasl_password=${KAFKA_SASL_PASS}
    ports:
    - ${KAFKA_EXTERNAL_HOST_PORT:-9092}:9092
    - ${KAFKA_INTERNAL_HOST_PORT:-29092}:29092
    - ${PANDAPROXY_EXTERNAL_HOST_PORT:-8082}:8082
    - ${PANDAPROXY_INTERNAL_HOST_PORT:-28082}:28082
    environment:
      KAFKA_SASL_MECHANISM: ${KAFKA_SASL_MECHANISM:-SCRAM-SHA-256}
      KAFKA_SASL_USER: ${KAFKA_SASL_USER:-redpanda}
      KAFKA_SASL_PASS: ${KAFKA_SASL_PASS:-admin}
    restart: unless-stopped
    profiles:
    - kafka
    healthcheck:
      test: [CMD, redpanda, admin, check]
      interval: 5s
      timeout: 1s
      retries: 30
    networks:
    - data_platform
    - redpanda_internal


  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:v2.2.2
    hostname: redpanda-console
    container_name: ${CONTAINER_PREFIX:-my}-redpanda-console
    ports:
    - ${REDPANDA_CONSOLE_HOST_PORT:-8081}:8080
    environment:
      KAFKA_BROKERS: redpanda:29092
    profiles:
    - kafka
    depends_on:
    - redpanda
    restart: unless-stopped
    networks:
    - redpanda_internal


  #############################################################################
  # POSTGRE
  #############################################################################

  postgresql:
    image: postgres:15.2
    hostname: postgresql
    container_name: ${CONTAINER_PREFIX:-my}-postgresql
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-admin}
      POSTGRES_DB: ${POSTGRES_DB:-mydb}
    volumes:
    - ./postgres-data:/var/lib/postgresql/data
    ports:
    - ${POSTGRES_HOST_PORT:-5432}:5432
    networks:
    - data_platform


  #############################################################################
  # CLICKHOUSE
  #############################################################################
  clickhouse-server:
    image: yandex/clickhouse-server
    hostname: clickhouse-server
    container_name: ${CONTAINER_PREFIX:-my}-clickhouse-server
    restart: unless-stopped
    environment:
      CLICKHOUSE_USER: ${CLICKHOUSE_USER:-clickhouse}
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-admin}
      CLICKHOUSE_DB: ${CLICKHOUSE_DB:-mydb}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    ports:
    - "${CLICKHOUSE_HOST_PORT:-8123}:8123"
    volumes:
    - ./clickhouse:/var/lib/clickhouse
    networks:
    - data_platform

  #############################################################################
  # METABASE
  #############################################################################
  metabase:
    # image: metabase/metabase:v0.46.6.1-latest-patch
    build: images/metabase
    hostname: metabase
    container_name: ${CONTAINER_PREFIX:-my}-metabase
    restart: unless-stopped
    volumes:
    - /dev/urandom:/dev/random:ro
    ports:
    - ${METABASE_HOST_PORT:-3000}:3000
    environment:
      MB_DB_TYPE: postgres
      MB_DB_DBNAME: metabase
      MB_DB_PORT: 5432
      MB_DB_USER: ${MB_POSTGRES_USER:-postgres}
      MB_DB_PASS: ${MB_POSTGRES_PASSWORD:-admin}
      MB_DB_HOST: metabase-postgres
    networks:
    - metabase_internal
    - data_platform
    healthcheck:
      test: curl --fail -I http://localhost:3000/api/health || exit 1
      interval: 15s
      timeout: 5s
      retries: 50
    depends_on:
    - metabase-postgres

  metabase-postgres:
    image: postgres:15.2
    hostname: metabase-postgres
    container_name: ${CONTAINER_PREFIX:-my}-metabase-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${MB_POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${MB_POSTGRES_PASSWORD:-admin}
      POSTGRES_DB: metabase
    volumes:
    - ./metabase/postgres-data:/var/lib/postgresql/data
    networks:
    - metabase_internal


  #############################################################################
  # AIRBYTE
  #############################################################################

  
  # hook in case we need to add init behavior
  # every root service (no depends_on) should depend on init
  airbyte-init:
    image: airbyte/init:${AIRBYTE_VERSION:-0.50.7}
    logging: *default-airbyte-logging
    hostname: airbyte-init
    container_name: ${CONTAINER_PREFIX:-my}-airbyte-init
    command: /bin/sh -c "./scripts/create_mount_directories.sh /local_parent ${AIRBYTE_HACK_LOCAL_ROOT_PARENT:-/tmp} ${AIRBYTE_LOCAL_ROOT:-/tmp/airbyte_local}"
    environment:
    - LOCAL_ROOT=${AIRBYTE_LOCAL_ROOT:-/tmp/airbyte_local}
    - HACK_LOCAL_ROOT_PARENT=${AIRBYTE_HACK_LOCAL_ROOT_PARENT:-/tmp}
    volumes:
    - ${AIRBYTE_HACK_LOCAL_ROOT_PARENT:-/tmp}:/local_parent

  airbyte-bootloader:
    image: airbyte/bootloader:${AIRBYTE_VERSION:-0.50.7}
    logging: *default-airbyte-logging
    hostname: airbyte-bootloader
    container_name: ${CONTAINER_PREFIX:-my}-airbyte-bootloader
    environment:
    - AIRBYTE_VERSION=${AIRBYTE_VERSION:-0.50.7}
    - DATABASE_PASSWORD=${AIRBYTE_DATABASE_PASSWORD:-docker}
    - DATABASE_URL=${AIRBYTE_DATABASE_URL:-jdbc:postgresql://airbyte-db:5432/airbyte}
    - DATABASE_USER=${AIRBYTE_DATABASE_USER:-docker}
    - LOG_LEVEL=${AIRBYTE_LOG_LEVEL:-INFO}
    - LOCAL_CONNECTOR_CATALOG_PATH=${AIRBYTE_LOCAL_CONNECTOR_CATALOG_PATH:-}
    networks:
    - airbyte_internal
    depends_on:
      airbyte-init:
        condition: service_completed_successfully

  airbyte-db:
    image: airbyte/db:${AIRBYTE_VERSION:-0.50.7}
    logging: *default-airbyte-logging
    hostname: airbyte-db
    container_name: ${CONTAINER_PREFIX:-my}-airbyte-db
    restart: unless-stopped
    environment:
    - CONFIG_DATABASE_PASSWORD=${AIRBYTE_CONFIG_DATABASE_PASSWORD:-}
    - CONFIG_DATABASE_URL=${AIRBYTE_CONFIG_DATABASE_URL:-}
    - CONFIG_DATABASE_USER=${AIRBYTE_CONFIG_DATABASE_USER:-}
    - DATABASE_PASSWORD=${AIRBYTE_DATABASE_PASSWORD:-docker}
    - DATABASE_URL=${AIRBYTE_DATABASE_URL:-jdbc:postgresql://airbyte-db:5432/airbyte}
    - DATABASE_USER=${AIRBYTE_DATABASE_USER:-docker}
    - POSTGRES_PASSWORD=${AIRBYTE_DATABASE_PASSWORD:-docker}
    - POSTGRES_USER=${AIRBYTE_DATABASE_USER:-docker}
    volumes:
    - ./airbyte/db:/var/lib/postgresql/data
    networks:
    - airbyte_internal

  airbyte-worker:
    image: airbyte/worker:${AIRBYTE_VERSION:-0.50.7}
    logging: *default-airbyte-logging
    hostname: airbyte-worker
    container_name: ${CONTAINER_PREFIX:-my}-airbyte-worker
    restart: unless-stopped
    environment:
    - AIRBYTE_VERSION=${AIRBYTE_VERSION:-0.50.7}
    - AUTO_DISABLE_FAILING_CONNECTIONS=${AIRBYTE_AUTO_DISABLE_FAILING_CONNECTIONS:-false}
    - CONFIG_DATABASE_PASSWORD=${AIRBYTE_CONFIG_DATABASE_PASSWORD:-}
    - CONFIG_DATABASE_URL=${AIRBYTE_CONFIG_DATABASE_URL:-}
    - CONFIG_DATABASE_USER=${AIRBYTE_CONFIG_DATABASE_USER:-}
    - CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION=${AIRBYTE_CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION:-0.40.23.002}
    - CONFIG_ROOT=${AIRBYTE_CONFIG_ROOT:-/data}
    - DATABASE_PASSWORD=${AIRBYTE_DATABASE_PASSWORD:-docker}
    - DATABASE_URL=${AIRBYTE_DATABASE_URL:-jdbc:postgresql://airbyte-db:5432/airbyte}
    - DATABASE_USER=${AIRBYTE_DATABASE_USER:-docker}
    - DEPLOYMENT_MODE=${DEPLOYMENT_MODE}
    - FEATURE_FLAG_CLIENT=${AIRBYTE_FEATURE_FLAG_CLIENT:-config}
    - LAUNCHDARKLY_KEY=${LAUNCHDARKLY_KEY}
    - INTERNAL_API_HOST=${AIRBYTE_INTERNAL_API_HOST:-airbyte-server:8001}
    - JOBS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION=${AIRBYTE_JOBS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION:-0.40.26.001}
    - JOB_MAIN_CONTAINER_CPU_LIMIT=${AIRBYTE_JOB_MAIN_CONTAINER_CPU_LIMIT:-}
    - JOB_MAIN_CONTAINER_CPU_REQUEST=${AIRBYTE_JOB_MAIN_CONTAINER_CPU_REQUEST:-}
    - JOB_MAIN_CONTAINER_MEMORY_LIMIT=${AIRBYTE_JOB_MAIN_CONTAINER_MEMORY_LIMIT:-}
    - JOB_MAIN_CONTAINER_MEMORY_REQUEST=${AIRBYTE_JOB_MAIN_CONTAINER_MEMORY_REQUEST:-}
    - LOCAL_DOCKER_MOUNT=${AIRBYTE_LOCAL_DOCKER_MOUNT:-/tmp/airbyte_local}
    - LOCAL_ROOT=${AIRBYTE_LOCAL_ROOT:-/tmp/airbyte_local}
    - LOG_LEVEL=${AIRBYTE_LOG_LEVEL:-INFO}
    - LOG_CONNECTOR_MESSAGES=${LOG_CONNECTOR_MESSAGES}
    - MAX_CHECK_WORKERS=${AIRBYTE_MAX_CHECK_WORKERS:-5}
    - MAX_DISCOVER_WORKERS=${AIRBYTE_MAX_DISCOVER_WORKERS:-5}
    - MAX_SPEC_WORKERS=${AIRBYTE_MAX_SPEC_WORKERS:-5}
    - MAX_SYNC_WORKERS=${AIRBYTE_MAX_SYNC_WORKERS:-5}
    - MAX_NOTIFY_WORKERS=${AIRBYTE_MAX_NOTIFY_WORKERS:-5}
    - SHOULD_RUN_NOTIFY_WORKFLOWS=${AIRBYTE_SHOULD_RUN_NOTIFY_WORKFLOWS:-true}
    - NORMALIZATION_JOB_MAIN_CONTAINER_MEMORY_LIMIT=${AIRBYTE_NORMALIZATION_JOB_MAIN_CONTAINER_MEMORY_LIMIT:-}
    - NORMALIZATION_JOB_MAIN_CONTAINER_MEMORY_REQUEST=${AIRBYTE_NORMALIZATION_JOB_MAIN_CONTAINER_MEMORY_REQUEST:-}
    - NORMALIZATION_JOB_MAIN_CONTAINER_CPU_LIMIT=${AIRBYTE_NORMALIZATION_JOB_MAIN_CONTAINER_CPU_LIMIT:-}
    - NORMALIZATION_JOB_MAIN_CONTAINER_CPU_REQUEST=${AIRBYTE_NORMALIZATION_JOB_MAIN_CONTAINER_CPU_REQUEST:-}
    - SECRET_PERSISTENCE=${SECRET_PERSISTENCE}
    - SYNC_JOB_MAX_ATTEMPTS=${AIRBYTE_SYNC_JOB_MAX_ATTEMPTS:-3}
    - SYNC_JOB_MAX_TIMEOUT_DAYS=${AIRBYTE_SYNC_JOB_MAX_TIMEOUT_DAYS:-3}
    - SYNC_JOB_INIT_RETRY_TIMEOUT_MINUTES=${AIRBYTE_SYNC_JOB_INIT_RETRY_TIMEOUT_MINUTES:-5}
    - TEMPORAL_HOST=${AIRBYTE_TEMPORAL_HOST:-airbyte-temporal:7233}
    - TRACKING_STRATEGY=${AIRBYTE_TRACKING_STRATEGY:-segment}
    - WEBAPP_URL=${AIRBYTE_WEBAPP_URL:-http://localhost:8000/}
    - WORKSPACE_DOCKER_MOUNT=${AIRBYTE_WORKSPACE_DOCKER_MOUNT:-airbyte_workspace}
    - WORKSPACE_ROOT=${AIRBYTE_WORKSPACE_ROOT:-/tmp/workspace}
    - METRIC_CLIENT=${AIRBYTE_METRIC_CLIENT:-}
    - OTEL_COLLECTOR_ENDPOINT=${AIRBYTE_OTEL_COLLECTOR_ENDPOINT:-http://host.docker.internal:4317}
    - JOB_ERROR_REPORTING_STRATEGY=${AIRBYTE_JOB_ERROR_REPORTING_STRATEGY:-logging}
    - JOB_ERROR_REPORTING_SENTRY_DSN=${JOB_ERROR_REPORTING_SENTRY_DSN}
    - ACTIVITY_MAX_ATTEMPT=${AIRBYTE_ACTIVITY_MAX_ATTEMPT:-}
    - ACTIVITY_INITIAL_DELAY_BETWEEN_ATTEMPTS_SECONDS=${AIRBYTE_ACTIVITY_INITIAL_DELAY_BETWEEN_ATTEMPTS_SECONDS:-}
    - ACTIVITY_MAX_DELAY_BETWEEN_ATTEMPTS_SECONDS=${AIRBYTE_ACTIVITY_MAX_DELAY_BETWEEN_ATTEMPTS_SECONDS:-}
    - WORKFLOW_FAILURE_RESTART_DELAY_SECONDS=${AIRBYTE_WORKFLOW_FAILURE_RESTART_DELAY_SECONDS:-}
    - AUTO_DETECT_SCHEMA=${AIRBYTE_AUTO_DETECT_SCHEMA:-true}
    - USE_STREAM_CAPABLE_STATE=${AIRBYTE_USE_STREAM_CAPABLE_STATE:-true}
    - MICRONAUT_ENVIRONMENTS=${AIRBYTE_WORKERS_MICRONAUT_ENVIRONMENTS:-control-plane}
    - APPLY_FIELD_SELECTION=${APPLY_FIELD_SELECTION}
    - FIELD_SELECTION_WORKSPACES=${FIELD_SELECTION_WORKSPACES}
    configs:
    - airbyte_flags
    volumes:
    - /var/run/docker.sock:/var/run/docker.sock
    - airbyte_workspace:${AIRBYTE_WORKSPACE_ROOT:-/tmp/workspace}
    - ${AIRBYTE_LOCAL_ROOT:-/tmp/airbyte_local}:${AIRBYTE_LOCAL_ROOT:-/tmp/airbyte_local}
    ports:
    - "${AIRBYTE_WORKER_HOST_PORT:-0}:9000"
    networks:
    - data_platform
    - airbyte_internal
    depends_on:
      airbyte-bootloader:
        condition: service_completed_successfully

  airbyte-server:
    image: airbyte/server:${AIRBYTE_VERSION:-0.50.7}
    logging: *default-airbyte-logging
    hostname: airbyte-server
    container_name: ${CONTAINER_PREFIX:-my}-airbyte-server
    restart: unless-stopped
    environment:
    - AIRBYTE_ROLE=${AIRBYTE_ROLE:-}
    - AIRBYTE_VERSION=${AIRBYTE_VERSION:-0.50.7}
    - CONFIG_DATABASE_PASSWORD=${AIRBYTE_CONFIG_DATABASE_PASSWORD:-}
    - CONFIG_DATABASE_URL=${AIRBYTE_CONFIG_DATABASE_URL:-}
    - CONFIG_DATABASE_USER=${AIRBYTE_CONFIG_DATABASE_USER:-}
    - CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION=${AIRBYTE_CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION:-0.40.23.002}
    - CONFIG_ROOT=${AIRBYTE_CONFIG_ROOT:-/data}
    - DATABASE_PASSWORD=${AIRBYTE_DATABASE_PASSWORD:-docker}
    - DATABASE_URL=${AIRBYTE_DATABASE_URL:-jdbc:postgresql://airbyte-db:5432/airbyte}
    - DATABASE_USER=${AIRBYTE_DATABASE_USER:-docker}
    - FEATURE_FLAG_CLIENT=${AIRBYTE_FEATURE_FLAG_CLIENT:-config}
    - LAUNCHDARKLY_KEY=${LAUNCHDARKLY_KEY}
    - JOB_MAIN_CONTAINER_CPU_LIMIT=${AIRBYTE_JOB_MAIN_CONTAINER_CPU_LIMIT:-}
    - JOB_MAIN_CONTAINER_CPU_REQUEST=${AIRBYTE_JOB_MAIN_CONTAINER_CPU_REQUEST:-}
    - JOB_MAIN_CONTAINER_MEMORY_LIMIT=${AIRBYTE_JOB_MAIN_CONTAINER_MEMORY_LIMIT:-}
    - JOB_MAIN_CONTAINER_MEMORY_REQUEST=${AIRBYTE_JOB_MAIN_CONTAINER_MEMORY_REQUEST:-}
    - JOBS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION=${AIRBYTE_JOBS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION:-0.40.26.001}
    - LOG_LEVEL=${AIRBYTE_LOG_LEVEL:-INFO}
    - NEW_SCHEDULER=${NEW_SCHEDULER}
    - SECRET_PERSISTENCE=${SECRET_PERSISTENCE}
    - TEMPORAL_HOST=${AIRBYTE_TEMPORAL_HOST:-airbyte-temporal:7233}
    - TRACKING_STRATEGY=${AIRBYTE_TRACKING_STRATEGY:-segment}
    - JOB_ERROR_REPORTING_STRATEGY=${AIRBYTE_JOB_ERROR_REPORTING_STRATEGY:-logging}
    - JOB_ERROR_REPORTING_SENTRY_DSN=${JOB_ERROR_REPORTING_SENTRY_DSN}
    - WEBAPP_URL=${AIRBYTE_WEBAPP_URL:-http://localhost:8000/}
    - WORKER_ENVIRONMENT=${WORKER_ENVIRONMENT}
    - WORKSPACE_ROOT=${AIRBYTE_WORKSPACE_ROOT:-/tmp/workspace}
    - GITHUB_STORE_BRANCH=${GITHUB_STORE_BRANCH}
    - MICRONAUT_ENVIRONMENTS=${AIRBYTE_WORKERS_MICRONAUT_ENVIRONMENTS:-control-plane}
    - AUTO_DETECT_SCHEMA=${AIRBYTE_AUTO_DETECT_SCHEMA:-true}
    - MAX_NOTIFY_WORKERS=5
    - SHOULD_RUN_NOTIFY_WORKFLOWS=${AIRBYTE_SHOULD_RUN_NOTIFY_WORKFLOWS:-true}
    ports:
    - "${AIRBYTE_SERVER_HOST_PORT:-0}:8001"
    configs:
    - airbyte_flags
    volumes:
    - airbyte_workspace:${AIRBYTE_WORKSPACE_ROOT:-/tmp/workspace}
    - airbyte_data:${AIRBYTE_CONFIG_ROOT:-/data}
    - ${AIRBYTE_LOCAL_ROOT:-/tmp/airbyte_local}:${AIRBYTE_LOCAL_ROOT:-/tmp/airbyte_local}
    networks:
    - data_platform
    - airbyte_internal
    depends_on:
      airbyte-bootloader:
        condition: service_completed_successfully

  airbyte-webapp:
    image: airbyte/webapp:${AIRBYTE_VERSION:-0.50.7}
    logging: *default-airbyte-logging
    hostname: airbyte-webapp
    container_name: ${CONTAINER_PREFIX:-my}-airbyte-webapp
    restart: unless-stopped
    ports:
    - "${AIRBYTE_WEBAPP_HOST_PORT:-0}:80"
    environment:
    - INTERNAL_API_HOST=${AIRBYTE_INTERNAL_API_HOST:-airbyte-server:8001}
    - CONNECTOR_BUILDER_API_HOST=${AIRBYTE_CONNECTOR_BUILDER_API_HOST:-airbyte-connector-builder-server:80}
    - TRACKING_STRATEGY=${AIRBYTE_TRACKING_STRATEGY:-segment}
    networks:
    - airbyte_internal
    depends_on:
      airbyte-bootloader:
        condition: service_completed_successfully

  airbyte-temporal:
    image: airbyte/temporal:${AIRBYTE_VERSION:-0.50.7}
    logging: *default-airbyte-logging
    hostname: airbyte-temporal
    container_name: ${CONTAINER_PREFIX:-my}-airbyte-temporal
    restart: unless-stopped
    environment:
    - DB=postgresql
    - DB_PORT=${AIRBYTE_DATABASE_PORT:-5432}
    - DYNAMIC_CONFIG_FILE_PATH=config/dynamicconfig/development.yaml
    - LOG_LEVEL=${AIRBYTE_LOG_LEVEL:-INFO}
    - POSTGRES_PWD=${AIRBYTE_DATABASE_PASSWORD:-docker}
    - POSTGRES_SEEDS=${AIRBYTE_DATABASE_HOST:-airbyte-db}
    - POSTGRES_USER=${AIRBYTE_DATABASE_USER:-docker}
    volumes:
    - ./airbyte/temporal/dynamicconfig:/etc/temporal/config/dynamicconfig
    networks:
    - airbyte_internal

  airbyte-cron:
    image: airbyte/cron:${AIRBYTE_VERSION:-0.50.7}
    logging: *default-airbyte-logging
    hostname: airbyte-cron
    container_name: ${CONTAINER_PREFIX:-my}-airbyte-cron
    restart: unless-stopped
    environment:
    - AIRBYTE_VERSION=${AIRBYTE_VERSION:-0.50.7}
    - CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION=${AIRBYTE_CONFIGS_DATABASE_MINIMUM_FLYWAY_MIGRATION_VERSION:-0.40.23.002}
    - DATABASE_PASSWORD=${AIRBYTE_DATABASE_PASSWORD:-docker}
    - DATABASE_URL=${AIRBYTE_DATABASE_URL:-jdbc:postgresql://airbyte-db:5432/airbyte}
    - DATABASE_USER=${AIRBYTE_DATABASE_USER:-docker}
    - DEPLOYMENT_MODE=${DEPLOYMENT_MODE}
    - LOG_LEVEL=${AIRBYTE_LOG_LEVEL:-INFO}
    - REMOTE_CONNECTOR_CATALOG_URL=${REMOTE_CONNECTOR_CATALOG_URL}
    - TEMPORAL_HISTORY_RETENTION_IN_DAYS=${TEMPORAL_HISTORY_RETENTION_IN_DAYS}
    - UPDATE_DEFINITIONS_CRON_ENABLED=${UPDATE_DEFINITIONS_CRON_ENABLED}
    - WORKSPACE_ROOT=${AIRBYTE_WORKSPACE_ROOT:-/tmp/workspace}
    - MICRONAUT_ENVIRONMENTS=${AIRBYTE_CRON_MICRONAUT_ENVIRONMENTS:-control-plane}
    configs:
    - airbyte_flags
    volumes:
    - airbyte_workspace:${AIRBYTE_WORKSPACE_ROOT:-/tmp/workspace}
    networks:
    - airbyte_internal
    depends_on:
      airbyte-bootloader:
        condition: service_completed_successfully

  airbyte-connector-builder-server:
    image: airbyte/connector-atelier-server:${AIRBYTE_VERSION:-0.50.7}
    logging: *default-airbyte-logging
    hostname: airbyte-connector-builder-server
    container_name: ${CONTAINER_PREFIX:-my}-airbyte-connector-builder-server
    restart: unless-stopped
    ports:
    - 80
    environment:
    - AIRBYTE_VERSION=${AIRBYTE_VERSION:-0.50.7}
    - CDK_VERSION=${CDK_VERSION}
    - DEPLOYMENT_MODE=${DEPLOYMENT_MODE}
    - PYTHON_VERSION=${PYTHON_VERSION}
    networks:
    - airbyte_internal
    depends_on:
      airbyte-bootloader:
        condition: service_completed_successfully

  airbyte-proxy:
    image: airbyte/proxy:${AIRBYTE_VERSION:-0.50.7}
    hostname: airbyte-proxy
    container_name: ${CONTAINER_PREFIX:-my}-airbyte-proxy
    restart: unless-stopped
    ports:
    - "${AIRBYTE_PROXY_HOST_PORT:-8000}:8000"
    - "8001:8001"
    - "8003:8003"
    environment:
    - BASIC_AUTH_USERNAME=${AIRBYTE_BASIC_AUTH_USERNAME:-airbyte}
    - BASIC_AUTH_PASSWORD=${AIRBYTE_BASIC_AUTH_PASSWORD:-admin}
    - BASIC_AUTH_PROXY_TIMEOUT=${AIRBYTE_BASIC_AUTH_PROXY_TIMEOUT:-900}
    networks:
    - airbyte_internal
    - airbyte_public
    depends_on:
    - airbyte-webapp
    - airbyte-server


  #############################################################################
  # AIRFLOW
  #############################################################################

  airflow-postgres:
    image: postgres:15.2
    hostname: airflow-postgres
    container_name: ${CONTAINER_PREFIX:-my}-airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
    - ./airflow/postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    networks:
      - airflow_internal
    restart: unless-stopped

  airflow-redis:
    image: redis:6.0.20
    hostname: airflow-redis
    container_name: ${CONTAINER_PREFIX:-my}-airflow-redis
    expose:
    - 6379
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 30s
      retries: 50
      start_period: 30s
    networks:
      - airflow_internal
    restart: unless-stopped

  airflow-webserver:
    <<: *airflow-common
    hostname: airflow-webserver
    container_name: ${CONTAINER_PREFIX:-my}-airflow-webserver
    command: webserver
    ports:
    - "${AIRFLOW_HOST_PORT:-8080}:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    hostname: airflow-scheduler
    container_name: ${CONTAINER_PREFIX:-my}-airflow-scheduler
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    hostname: airflow-worker
    container_name: ${CONTAINER_PREFIX:-my}-airflow-worker
    command: celery worker
    healthcheck:
      test:
      - "CMD-SHELL"
      - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    environment:
      <<: *airflow-common-env
      # Required to handle warm shutdown of the celery workers properly
      # See https://airflow.apache.org/docs/docker-stack/entrypoint.html#signal-propagation
      DUMB_INIT_SETSID: "0"
    restart: unless-stopped
    networks:
    - airflow_internal
    - data_platform
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    hostname: airflow-triggerer
    container_name: ${CONTAINER_PREFIX:-my}-airflow-triggerer
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    hostname: airflow-init
    container_name: ${CONTAINER_PREFIX:-my}-airflow-init
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
    - -c
    - |
        function ver() {
          printf "%04d%04d%04d%04d" $${1//./ }
        }
        airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO && gosu airflow airflow version)
        airflow_version_comparable=$$(ver $${airflow_version})
        min_airflow_version=2.2.0
        min_airflow_version_comparable=$$(ver $${min_airflow_version})
        if (( airflow_version_comparable < min_airflow_version_comparable )); then
          echo
          echo -e "\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m"
          echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"
          echo
          exit 1
        fi
        if [[ -z "${AIRFLOW_UID:-50000}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID:-50000}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-admin}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
    - ./airflow:/sources

  airflow-cli:
    <<: *airflow-common
    hostname: airflow-cli
    container_name: ${CONTAINER_PREFIX:-my}-airflow-cli
    profiles:
    - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: "0"
    # Workaround for entrypoint issue. See: https://github.com/apache/airflow/issues/16252
    command:
    - bash
    - -c
    - airflow

  # You can enable flower by adding "--profile flower" option e.g. docker-compose --profile flower up
  # or by explicitly targeted on the command line e.g. docker-compose up flower.
  # See: https://docs.docker.com/compose/profiles/
  airflow-flower:
    <<: *airflow-common
    hostname: airflow-flower
    container_name: ${CONTAINER_PREFIX:-my}-airflow-flower
    command: celery flower
    profiles:
    - flower
    ports:
    - "${AIRFLOW_FLOWER_HOST_PORT:-5555}:5555"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
